name: NCAA Basketball Scraper (Docker)

on:
  # Run daily at 10 AM UTC (scraping)
  schedule:
    - cron: '0 10 * * *'
  # Run daily at 2 PM UTC (retry failed games 4 hours later)
    - cron: '0 14 * * *'
  
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      date:
        description: 'Date to scrape (YYYY/MM/DD format, leave empty for yesterday)'
        required: false
        type: string
      force_rescrape:
        description: 'Force rescrape and override existing Google Drive files'
        required: false
        default: false
        type: boolean
      retry_only:
        description: 'Only run retry job for failed games (skip discovery and scrape)'
        required: false
        default: false
        type: boolean

env:
  PYTHONUNBUFFERED: 1
  OUTPUT_DIR: /app/data
  DISPLAY: :99
  CHROME_BIN: /usr/bin/google-chrome
  CHROME_PATH: /usr/bin/google-chrome
  DOCKER_CONTAINER: true
  WDM_LOG_LEVEL: 0
  WDM_LOCAL: 1
  UPLOAD_TO_GDRIVE: true

jobs:
  check-schedule-time:
    runs-on: ubuntu-latest
    outputs:
      is_morning_run: ${{ steps.check.outputs.is_morning }}
    steps:
      - name: Check UTC hour
        id: check
        run: |
          if [ "${{ github.event_name }}" = "schedule" ]; then
            HOUR=$(date -u +%H)
            if [ "$HOUR" -ge 10 ] && [ "$HOUR" -lt 14 ]; then
              echo "is_morning=true" >> $GITHUB_OUTPUT
            else
              echo "is_morning=false" >> $GITHUB_OUTPUT
            fi
            echo "Current UTC hour: $HOUR"
          else
            # For workflow_dispatch, default to morning run (full scrape)
            echo "is_morning=true" >> $GITHUB_OUTPUT
            echo "Not a schedule event, defaulting to morning run"
          fi

  discover-games:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Discovery should be fast
    needs: check-schedule-time
    if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.retry_only != 'true') || (github.event_name == 'schedule' && needs.check-schedule-time.outputs.is_morning_run == 'true') }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: ncaa-scraper:latest
        load: true
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Provide OAuth token
      shell: bash
      run: |
        if [ -n "${{ secrets.GOOGLE_TOKEN_FILE_B64 }}" ]; then
          printf '%s' '${{ secrets.GOOGLE_TOKEN_FILE_B64 }}' | base64 -d > token.pickle
        else
          echo "GOOGLE_TOKEN_FILE_B64 secret is missing" >&2; exit 1
        fi

        cat > .env << EOF
        GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}
        GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}
        GOOGLE_REDIRECT_URI=${{ secrets.GOOGLE_REDIRECT_URI }}
        GOOGLE_DRIVE_FOLDER_ID=${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        DISCORD_WEBHOOK_URL=${{ secrets.DISCORD_WEBHOOK_URL }}
        OUTPUT_DIR=/app/data
        LOG_LEVEL=INFO
        UPLOAD_TO_GDRIVE=true
        GOOGLE_TOKEN_FILE=/app/token.pickle
        EOF
        
    - name: Create directories
      run: |
        mkdir -p data logs discovery
        chmod 777 data logs discovery
        
    - name: Run Discovery
      run: |
        DATE_ARG=""
        if [ -n "${{ github.event.inputs.date }}" ]; then
          DATE_ARG="--date ${{ github.event.inputs.date }}"
        fi
        
        docker run --rm \
          --name ncaa-scraper-discovery \
          --shm-size=2gb \
          --memory=4g \
          --memory-swap=4g \
          -v $(pwd)/discovery:/app/discovery \
          -v $(pwd)/.env:/app/.env \
          -v $(pwd)/token.pickle:/app/token.pickle \
          -e PYTHONUNBUFFERED=1 \
          -e OUTPUT_DIR=/app/data \
          -e DISPLAY=:99 \
          -e CHROME_BIN=/usr/bin/google-chrome \
          -e CHROME_PATH=/usr/bin/google-chrome \
          -e DOCKER_CONTAINER=true \
          -e WDM_LOG_LEVEL=0 \
          -e WDM_LOCAL=1 \
          -e UPLOAD_TO_GDRIVE=true \
          ncaa-scraper:latest \
          --discover $DATE_ARG
        
    - name: Upload discovery mapping
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: game-links-mapping
        path: discovery/game_links_mapping.json
        retention-days: 1
        
    - name: Cleanup
      if: always()
      run: |
        rm -f credentials.json .env token.pickle
        docker container prune -f

  scrape:
    needs: [discover-games, check-schedule-time]
    runs-on: ubuntu-latest
    timeout-minutes: 120  # Each job should be much shorter now
    if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.retry_only != 'true') || (github.event_name == 'schedule' && needs.check-schedule-time.outputs.is_morning_run == 'true') }}
    strategy:
      matrix:
        include:
          - division: d1
            gender: men
          - division: d1
            gender: women
          - division: d2
            gender: men
          - division: d2
            gender: women
          - division: d3
            gender: men
          - division: d3
            gender: women
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: ncaa-scraper:latest
        load: true
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Download discovery mapping
      uses: actions/download-artifact@v4
      with:
        name: game-links-mapping
        path: discovery/
        
    - name: Provide OAuth token
      shell: bash
      run: |
        if [ -n "${{ secrets.GOOGLE_TOKEN_FILE_B64 }}" ]; then
          printf '%s' '${{ secrets.GOOGLE_TOKEN_FILE_B64 }}' | base64 -d > token.pickle
        else
          echo "GOOGLE_TOKEN_FILE_B64 secret is missing" >&2; exit 1
        fi

        cat > .env << EOF
        GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}
        GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}
        GOOGLE_REDIRECT_URI=${{ secrets.GOOGLE_REDIRECT_URI }}
        GOOGLE_DRIVE_FOLDER_ID=${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        DISCORD_WEBHOOK_URL=${{ secrets.DISCORD_WEBHOOK_URL }}
        OUTPUT_DIR=/app/data
        LOG_LEVEL=INFO
        UPLOAD_TO_GDRIVE=true
        GOOGLE_TOKEN_FILE=/app/token.pickle
        EOF
        
    - name: Create directories
      run: |
        mkdir -p data logs
        chmod 777 data logs
        touch failed_games.json
        chmod 666 failed_games.json
        
    - name: Run Scraper for ${{ matrix.division }} ${{ matrix.gender }}
      run: |
        DATE_ARG=""
        if [ -n "${{ github.event.inputs.date }}" ]; then
          DATE_ARG="--date ${{ github.event.inputs.date }}"
        fi
        
        FORCE_ARG=""
        if [ "${{ github.event.inputs.force_rescrape }}" = "true" ]; then
          FORCE_ARG="--force-rescrape"
        fi
        
        docker run --rm \
          --name ncaa-scraper-${{ matrix.division }}-${{ matrix.gender }} \
          --shm-size=2gb \
          --memory=4g \
          --memory-swap=4g \
          -v $(pwd)/data:/app/data \
          -v $(pwd)/logs:/app/logs \
          -v $(pwd)/discovery:/app/discovery \
          -v $(pwd)/failed_games.json:/app/failed_games.json \
          -v $(pwd)/.env:/app/.env \
          -v $(pwd)/token.pickle:/app/token.pickle \
          -e PYTHONUNBUFFERED=1 \
          -e OUTPUT_DIR=/app/data \
          -e DISPLAY=:99 \
          -e CHROME_BIN=/usr/bin/google-chrome \
          -e CHROME_PATH=/usr/bin/google-chrome \
          -e DOCKER_CONTAINER=true \
          -e WDM_LOG_LEVEL=0 \
          -e WDM_LOCAL=1 \
          -e UPLOAD_TO_GDRIVE=true \
          ncaa-scraper:latest \
          --single-division ${{ matrix.division }} \
          --single-gender ${{ matrix.gender }} \
          --mapping-file /app/discovery/game_links_mapping.json \
          --failed-games-file /app/failed_games.json \
          $DATE_ARG \
          $FORCE_ARG
        
    - name: Upload scraped data
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraped-data-${{ matrix.division }}-${{ matrix.gender }}-${{ github.run_number }}
        path: data/
        retention-days: 30
        
    - name: Upload logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: logs-${{ matrix.division }}-${{ matrix.gender }}-${{ github.run_number }}
        path: logs/
        retention-days: 7
        
    - name: Upload failed games
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: failed-games-${{ matrix.division }}-${{ matrix.gender }}-${{ github.run_number }}
        path: failed_games.json
        retention-days: 7
        if-no-files-found: ignore
        
    - name: Cleanup
      if: always()
      run: |
        rm -f credentials.json .env token.pickle
        docker container prune -f

  retry-failed:
    needs: [scrape, check-schedule-time]
    runs-on: ubuntu-latest
    timeout-minutes: 120
    if: always() && (github.event_name == 'workflow_dispatch' && github.event.inputs.retry_only != 'true')  # Only run on manual dispatch, not on scheduled runs
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: ncaa-scraper:latest
        load: true
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Download failed games artifacts
      uses: actions/download-artifact@v4
      if: always()
      continue-on-error: true
      with:
        pattern: failed-games-*
        merge-multiple: true
        path: ./
        
    - name: Merge failed games files
      shell: bash
      run: |
        # Merge all failed games JSON files into one
        python3 << 'EOF'
        import json
        import glob
        from pathlib import Path
        
        merged = {}
        failed_files = glob.glob("failed-games-*/failed_games.json")
        
        if not failed_files:
            print("No failed games files found, creating empty file")
            Path("failed_games.json").touch()
            exit(0)
        
        print(f"Found {len(failed_files)} failed games files to merge")
        
        for file_path in failed_files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    # Merge by date
                    for date_key, games in data.items():
                        if date_key not in merged:
                            merged[date_key] = {}
                        # Merge games, avoiding duplicates
                        for game_link, entries in games.items():
                            if game_link not in merged[date_key]:
                                merged[date_key][game_link] = []
                            # Add entries that don't already exist
                            existing_combos = {(e['division'], e['gender']) for e in merged[date_key][game_link]}
                            for entry in entries:
                                combo = (entry['division'], entry['gender'])
                                if combo not in existing_combos:
                                    merged[date_key][game_link].append(entry)
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
                continue
        
        # Write merged file
        with open("failed_games.json", 'w') as f:
            json.dump(merged, f, indent=2)
        
        total_failed = sum(len(entries) for games in merged.values() for entries in games.values())
        print(f"Merged {total_failed} failed game entries into failed_games.json")
        EOF
        
    - name: Provide OAuth token
      shell: bash
      run: |
        if [ -n "${{ secrets.GOOGLE_TOKEN_FILE_B64 }}" ]; then
          printf '%s' '${{ secrets.GOOGLE_TOKEN_FILE_B64 }}' | base64 -d > token.pickle
        else
          echo "GOOGLE_TOKEN_FILE_B64 secret is missing" >&2; exit 1
        fi

        cat > .env << EOF
        GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}
        GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}
        GOOGLE_REDIRECT_URI=${{ secrets.GOOGLE_REDIRECT_URI }}
        GOOGLE_DRIVE_FOLDER_ID=${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        DISCORD_WEBHOOK_URL=${{ secrets.DISCORD_WEBHOOK_URL }}
        OUTPUT_DIR=/app/data
        LOG_LEVEL=INFO
        UPLOAD_TO_GDRIVE=true
        GOOGLE_TOKEN_FILE=/app/token.pickle
        EOF
        
    - name: Create directories
      run: |
        mkdir -p data logs
        chmod 777 data logs
        
    - name: Retry failed games
      run: |
        DATE_ARG=""
        if [ -n "${{ github.event.inputs.date }}" ]; then
          DATE_ARG="--date ${{ github.event.inputs.date }}"
        fi
        
        FORCE_ARG=""
        if [ "${{ github.event.inputs.force_rescrape }}" = "true" ]; then
          FORCE_ARG="--force-rescrape"
        fi
        
        # Check if failed games file exists and has content
        if [ -f "failed_games.json" ] && [ -s "failed_games.json" ]; then
          echo "Retrying failed games..."
          docker run --rm \
            --name ncaa-scraper-retry \
            --shm-size=2gb \
            --memory=4g \
            --memory-swap=4g \
            -v $(pwd)/data:/app/data \
            -v $(pwd)/logs:/app/logs \
            -v $(pwd)/failed_games.json:/app/failed_games.json \
            -v $(pwd)/.env:/app/.env \
            -v $(pwd)/token.pickle:/app/token.pickle \
            -e PYTHONUNBUFFERED=1 \
            -e OUTPUT_DIR=/app/data \
            -e DISPLAY=:99 \
            -e CHROME_BIN=/usr/bin/google-chrome \
            -e CHROME_PATH=/usr/bin/google-chrome \
            -e DOCKER_CONTAINER=true \
            -e WDM_LOG_LEVEL=0 \
            -e WDM_LOCAL=1 \
            -e UPLOAD_TO_GDRIVE=true \
            ncaa-scraper:latest \
            --retry-failed \
            --failed-games-file /app/failed_games.json \
            $DATE_ARG \
            $FORCE_ARG
        else
          echo "No failed games file found or file is empty, skipping retry"
        fi
        
    - name: Upload retry logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: retry-logs-${{ github.run_number }}
        path: logs/
        retention-days: 7
        if-no-files-found: ignore
        
    - name: Cleanup
      if: always()
      run: |
        rm -f credentials.json .env token.pickle
        docker container prune -f

  retry-only:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    needs: check-schedule-time
    if: github.event.inputs.retry_only == 'true' || (github.event_name == 'schedule' && needs.check-schedule-time.outputs.is_morning_run == 'false')  # Run when retry_only is enabled OR on afternoon scheduled run (14:00 UTC)
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: ncaa-scraper:latest
        load: true
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Find and download failed games artifacts from previous run
      uses: actions/github-script@v7
      continue-on-error: true
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          const path = require('path');
          const https = require('https');
          
          // List all workflow runs (this works!)
          const runs = await github.rest.actions.listWorkflowRunsForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            per_page: 10
          });
          
          // Find the most recent completed run before this one
          let previousRun = null;
          for (const run of runs.data.workflow_runs) {
            if (run.id < ${{ github.run_id }} && run.status === 'completed') {
              previousRun = run;
              break;
            }
          }
          
          if (!previousRun) {
            console.log('No previous completed run found');
            return;
          }
          
          console.log(`Found previous run: ${previousRun.id} (${previousRun.conclusion})`);
          
          // Use the artifacts_url from the workflow run response
          // GET /repos/{owner}/{repo}/actions/runs/{run_id}/artifacts
          const artifactsResponse = await github.request(previousRun.artifacts_url);
          
          // Filter for failed-games artifacts
          const failedGamesArtifacts = artifactsResponse.data.artifacts.filter(
            artifact => artifact.name.startsWith('failed-games-')
          );
          
          if (failedGamesArtifacts.length === 0) {
            console.log('No failed-games artifacts found in previous run');
            return;
          }
          
          console.log(`Found ${failedGamesArtifacts.length} failed-games artifacts`);
          
          // Download each artifact
          const downloadDir = './failed-games-artifacts';
          if (!fs.existsSync(downloadDir)) {
            fs.mkdirSync(downloadDir, { recursive: true });
          }
          
          // Helper function to download file following redirects
          const downloadFile = (url, dest, token, isRedirect = false) => {
            return new Promise((resolve, reject) => {
              const file = fs.createWriteStream(dest);
              
              // Only send auth headers to GitHub API, not to redirect URLs
              const headers = {
                'Accept': 'application/vnd.github+json',
                'X-GitHub-Api-Version': '2022-11-28',
                'User-Agent': 'GitHub-Actions'
              };
              
              // Only add Authorization header for initial GitHub API request
              if (!isRedirect) {
                headers['Authorization'] = `Bearer ${token}`;
              }
              
              const request = https.get(url, {
                headers: headers,
                followRedirect: false  // Handle redirects manually
              }, (response) => {
                // Handle redirects (302) - look for Location header as per GitHub API docs
                if (response.statusCode === 302 || response.statusCode === 301) {
                  // Node.js normalizes headers to lowercase
                  const redirectUrl = response.headers.location || response.headers.Location;
                  
                  if (!redirectUrl) {
                    file.close();
                    fs.unlinkSync(dest);
                    return reject(new Error('No Location header in redirect response'));
                  }
                  
                  // Consume the response body to allow connection to close
                  response.resume();
                  
                  file.close();
                  fs.unlinkSync(dest);
                  // Follow redirect without auth header (redirect URL is signed and expires in 1 minute)
                  return downloadFile(redirectUrl, dest, token, true).then(resolve).catch(reject);
                }
                
                if (response.statusCode !== 200) {
                  // Consume response body on error
                  response.resume();
                  file.close();
                  fs.unlinkSync(dest);
                  return reject(new Error(`Failed to download: ${response.statusCode}`));
                }
                
                response.pipe(file);
                file.on('finish', () => {
                  file.close();
                  resolve();
                });
              });
              
              request.on('error', (err) => {
                file.close();
                if (fs.existsSync(dest)) {
                  fs.unlinkSync(dest);
                }
                reject(err);
              });
            });
          };
          
          for (const artifact of failedGamesArtifacts) {
            console.log(`Downloading artifact: ${artifact.name} (ID: ${artifact.id})`);
            
            // Download artifact following GitHub API docs
            // GET /repos/{owner}/{repo}/actions/artifacts/{artifact_id}/zip
            // This returns a 302 redirect, which we need to follow
            const downloadUrl = `https://api.github.com/repos/${context.repo.owner}/${context.repo.repo}/actions/artifacts/${artifact.id}/zip`;
            
            const zipPath = path.join(downloadDir, `${artifact.name}.zip`);
            
            try {
              await downloadFile(downloadUrl, zipPath, process.env.GITHUB_TOKEN);
              
              // Extract using unzip
              const { execSync } = require('child_process');
              execSync(`unzip -q -o "${zipPath}" -d "${downloadDir}/${artifact.name}"`);
              console.log(`Extracted ${artifact.name}`);
            } catch (error) {
              console.error(`Error downloading/extracting ${artifact.name}: ${error.message}`);
            }
          }
          
          console.log('All artifacts downloaded and extracted');
        
    - name: Merge failed games files
      shell: bash
      run: |
        # Merge all failed games JSON files into one
        python3 << 'EOF'
        import json
        import glob
        from pathlib import Path
        
        merged = {}
        # Look in both current directory and artifacts directory (including subdirectories)
        failed_files = (
            glob.glob("failed-games-*/failed_games.json") + 
            glob.glob("failed-games-artifacts/**/failed_games.json", recursive=True)
        )
        
        if not failed_files:
            print("No failed games files found from artifacts")
            # Try to load from a previous workflow run's artifact
            # For now, create empty file - user can manually upload failed_games.json if needed
            Path("failed_games.json").touch()
            print("Created empty failed_games.json - if you have a failed_games.json file, upload it as an artifact first")
            exit(0)
        
        print(f"Found {len(failed_files)} failed games files to merge")
        
        for file_path in failed_files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    # Merge by date
                    for date_key, games in data.items():
                        if date_key not in merged:
                            merged[date_key] = {}
                        # Merge games, avoiding duplicates
                        for game_link, entries in games.items():
                            if game_link not in merged[date_key]:
                                merged[date_key][game_link] = []
                            # Add entries that don't already exist
                            existing_combos = {(e['division'], e['gender']) for e in merged[date_key][game_link]}
                            for entry in entries:
                                combo = (entry['division'], entry['gender'])
                                if combo not in existing_combos:
                                    merged[date_key][game_link].append(entry)
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
                continue
        
        # Write merged file
        with open("failed_games.json", 'w') as f:
            json.dump(merged, f, indent=2)
        
        total_failed = sum(len(entries) for games in merged.values() for entries in games.values())
        print(f"Merged {total_failed} failed game entries into failed_games.json")
        EOF
        
    - name: Provide OAuth token
      shell: bash
      run: |
        if [ -n "${{ secrets.GOOGLE_TOKEN_FILE_B64 }}" ]; then
          printf '%s' '${{ secrets.GOOGLE_TOKEN_FILE_B64 }}' | base64 -d > token.pickle
        else
          echo "GOOGLE_TOKEN_FILE_B64 secret is missing" >&2; exit 1
        fi

        cat > .env << EOF
        GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}
        GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}
        GOOGLE_REDIRECT_URI=${{ secrets.GOOGLE_REDIRECT_URI }}
        GOOGLE_DRIVE_FOLDER_ID=${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        DISCORD_WEBHOOK_URL=${{ secrets.DISCORD_WEBHOOK_URL }}
        OUTPUT_DIR=/app/data
        LOG_LEVEL=INFO
        UPLOAD_TO_GDRIVE=true
        GOOGLE_TOKEN_FILE=/app/token.pickle
        EOF
        
    - name: Create directories
      run: |
        mkdir -p data logs
        chmod 777 data logs
        
    - name: Retry failed games
      run: |
        DATE_ARG=""
        if [ -n "${{ github.event.inputs.date }}" ]; then
          DATE_ARG="--date ${{ github.event.inputs.date }}"
        fi
        
        FORCE_ARG=""
        if [ "${{ github.event.inputs.force_rescrape }}" = "true" ]; then
          FORCE_ARG="--force-rescrape"
        fi
        
        # Check if failed games file exists and has content
        if [ -f "failed_games.json" ] && [ -s "failed_games.json" ]; then
          echo "Retrying failed games..."
          docker run --rm \
            --name ncaa-scraper-retry \
            --shm-size=2gb \
            --memory=4g \
            --memory-swap=4g \
            -v $(pwd)/data:/app/data \
            -v $(pwd)/logs:/app/logs \
            -v $(pwd)/failed_games.json:/app/failed_games.json \
            -v $(pwd)/.env:/app/.env \
            -v $(pwd)/token.pickle:/app/token.pickle \
            -e PYTHONUNBUFFERED=1 \
            -e OUTPUT_DIR=/app/data \
            -e DISPLAY=:99 \
            -e CHROME_BIN=/usr/bin/google-chrome \
            -e CHROME_PATH=/usr/bin/google-chrome \
            -e DOCKER_CONTAINER=true \
            -e WDM_LOG_LEVEL=0 \
            -e WDM_LOCAL=1 \
            -e UPLOAD_TO_GDRIVE=true \
            ncaa-scraper:latest \
            --retry-failed \
            --failed-games-file /app/failed_games.json \
            $DATE_ARG \
            $FORCE_ARG
        else
          echo "No failed games file found or file is empty"
          echo "Please ensure failed games artifacts from previous runs are available,"
          echo "or manually upload a failed_games.json file as an artifact"
          exit 1
        fi
        
    - name: Upload retry logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: retry-only-logs-${{ github.run_number }}
        path: logs/
        retention-days: 7
        if-no-files-found: ignore
        
    - name: Cleanup
      if: always()
      run: |
        rm -f credentials.json .env token.pickle
        docker container prune -f
